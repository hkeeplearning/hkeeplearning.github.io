---
title: 深度学习基础
date: 2020-05-05 23:11:48
categories:
	- 深度学习
tags:
	- PyTorch
	- 深度学习基础
---
深度学习所用到的数学知识并没有特别高深，其中最重要的是损失函数，梯度下降和激活函数。

<!-- more -->

## 损失函数

损失函数使用来估量模型和预测值和真实值之间的不一样程度，它是一个非负的实数。损失函数的值越小，表示模型越好。

### 均方差损失函数

$$
MSE =
$$

### 绝对误差

$$
MAE=
$$

### 交叉熵损失函数

概率估计上定义，

## 梯度下降

### SGD



### RMSProp



### Adam



## 激活函数

在神经网络的计算过程中，每层都相当于一个矩阵相乘，所以无论多少层的神经网络都是输入的线性组合。所以需要激活函数来引入非线性的因素，使得神经网络可以逼近任何非线性函数，增加模型的泛化特性。

### sigmoid

$$
s(x) = \frac{1}{1 + {e}^{-x}}
$$

导数为：
$$
\frac {\partial s(x)}{\partial x} = s(x)(1-s(x))
$$
- sigmoid需要进行指数运算，因此计算速度比较慢
- *函数的输出不是以0为中心，这样会使权重更新效率降低（Why？）*，[参考](https://liam.page/2018/04/17/zero-centered-active-function/)

更新效率即指函数的收敛速度，迭代次数越多，说明模型的收敛速度越慢；反之，迭代次数少，则表明收敛速度快。

深度学习的一般方法即方向传播，即链式法则。
$$
w \leftarrow w - \eta \frac {\partial L}{\partial w}
$$
对某一个神经元，其输入输出关系是
$$
f(\vec x;\vec w, b) = f(\sum w_i x_i + b)
$$
因此，对某一个参数$w_i$来说
$$
\frac {\partial L}{\partial w_i} = \frac {\partial L}{\partial f} \frac {\partial f}{\partial z} \frac {\partial z}{\partial w_i} = x_i \frac {\partial L}{\partial f} \frac {\partial f}{\partial z}
$$
因此，对于某一个参数，其更新过程为
$$
w_i \leftarrow w_i - \eta \frac {\partial L}{\partial w_i} = w_i - \eta x_i \frac {\partial L}{\partial f} \frac {\partial f}{\partial z}
$$

参数$w_i$的实际更新方向由$\frac {\partial L}{\partial f} \frac {\partial f}{\partial z}$决定。另外，从上式也可以看出对于任意一个$w_i$，$\frac {\partial L}{\partial f} \frac {\partial f}{\partial z}$都是常数，所以各个$w_i$更新方向的差异，完全由输入值$x_i$决定。

**以零为中心的影响：**

以二维输入为例
$$
f(\vec x;\vec w, b) = f(w_0 x_0 + w_1 x_1 + b)
$$
现在假设参数$w_0$，$w_1$的最优解是$w_0^*$，$w_1^*$，且满足条件
$$
w_0 \lt w_0^*, w_1 \ge w_1^*
$$
这说明，$w_0$希望增大，而$w_1$需要适当减小。因此要求$x_0$和$x_1$的符号相反，但是在sigmoid函数中，输出值始终未正。这也就是说，如果上一级神经元采用sigmoid函数作为激活函数，那么我们总无法做到$x_0$和$x_1$符号相反。

### tanh


### relu


### leaky relu