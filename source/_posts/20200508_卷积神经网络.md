---
title: 卷积神经网络
categories:
  - 深度学习
tags:
  - PyTorch
  - CNN
abbrlink: 3329651476
date: 2020-05-06 15:11:48
---

卷积神经网络是由一个或者多个卷积层和一个全连接层组成的前馈神经网络。特别适合于提取图像特征。

<!-- more -->

## 卷积神经网络的必要性

- 减少图像处理的计算量

- 卷积的特性特别适合于提取图像特征

在一幅图像中，

1. 特征通常在很小的一个区域

2. 相同的一个特征可能出现在图像中的不同位置

3. 图像的subsampling不影响对这幅图像的理解

因此我们可以简化模型参数。

## 卷积神经网络的结构

1. 卷积层（convolution layer）
2. 激活函数（卷积的操作也是线性的，需要激活函数，引入非线性的因素）
3. 池化（max pooling）
4. dropout
5. 压扁（flatten）
6. 全连接（fully connected feedforward network）

1, 2, 3过程可以重复很多次。

对影像的处理：第一，要生成一个pattern，不需要看整张的image，你只需要看image的一小部分；第二，通用的pattern会出现在一张image的不同的区域；第三，我们可以对image做subsampling。

### 卷积层

卷积核大小：$f$

图像边界填充的的大小：$p$

卷积核滑动的步长：$s$

输入矩阵的大小：$n$

那么卷积后输出的个数为：
$$
\frac {n + 2 * p - f}{s} + 1 向下取整
$$

对image，移动卷积核1可以得到经过卷积后的矩阵。

![卷积核1](/images/20200508/chapter21-11.png)

不同的卷积核，提取图像的不同特征，设有另外的一个卷积核2，经过运算后得到了另外一个$4 \times 4$的矩阵。红色和蓝色的矩阵合起来叫feature map。有多少个卷积核，就会得到多少个特征。（如果有100个卷积核，就会得到一个$100 \times 4 \times 4$的立方体。）

![卷积核2](/images/20200508/chapter21-12.png)

convolution相当于fully connected的某些参数共享权重。

### 池化

池化后，每一个卷积输出的矩阵的大小为：
$$
\frac {n - f} {s} + 1
$$
这里的$n$是卷积输出的矩阵的大小。

max pooling的过程

![max pooling过程](/images/20200508/chapter21-18.png)

max pooling的结果

![max pooling结果](/images/20200508/chapter21-19.png)

设有100个卷积核，经过max pooling后就得到了一个$100 \times 2 \times2$立方体。一个卷积核就代表一个channel，即有100个channel。那么下一个卷积核的大小就是$100 \times 3 \times 3$？。

### Flatten

![flatten](/images/20200508/chapter21-22.png)

### 全连接

由于我们的特征都是矩阵的形式，因此全连接前，还要把特征进行压扁，将这些特征变成一维的向量。

如果是分类，使用`softmax`作为输出；如果是回归的问题，直接使用`linear`。

![CNN一个完整的例子](/images/20200508/chapter21-25.png)

## 参考

- [为什么要用CNN](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)

- [LeNet-5, convolutional neural networks](http://yann.lecun.com/exdb/lenet/index.html)
- [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
- [VGG](https://arxiv.org/pdf/1409.1556.pdf)
- [GoogLeNet ](https://arxiv.org/abs/1512.00567)
- [ResNet](https://arxiv.org/abs/1512.03385)