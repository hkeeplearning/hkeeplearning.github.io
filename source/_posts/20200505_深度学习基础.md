---
title: 深度学习基础
categories:
  - 深度学习
tags:
  - PyTorch
  - 理论
abbrlink: 3233011741
date: 2020-05-05 23:11:48
---
深度学习所用到的数学知识并没有特别高深，其中最重要的是损失函数，梯度下降和激活函数。

<!-- more -->

## 损失函数

损失函数使用来估量模型和预测值和真实值之间的不一样程度，它是一个非负的实数。损失函数的值越小，表示模型越好。

### 均方差损失函数

$$
MSE =
$$

### 绝对误差

$$
MAE=
$$

### 交叉熵损失函数

概率估计上定义，
$$
CrossEntropy=
$$
多分类

## 梯度下降

### SGD

$$
w \leftarrow w - \eta \frac{\partial L}{\partial w}
$$



**Momentum**
$$
\nabla = \lambda {\nabla}_{i - 1} + \eta {\nabla}_{i}
$$

$$
w \leftarrow w - \nabla
$$


### RMSProp

$$
E[\nabla ^2] = 0.9E[\nabla ^2]_{i - 1} + 0.1 \nabla _i^2
$$

$$
w \leftarrow w - \frac{\eta}{\sqrt{E[\nabla ^2] + p}} \nabla_i
$$



### Adam

**相当于 RMSprop + Momentum**



## 激活函数

在神经网络的计算过程中，每层都相当于一个矩阵相乘，所以无论多少层的神经网络都是输入的线性组合。所以需要激活函数来引入非线性的因素，使得神经网络可以逼近任何非线性函数，增加模型的泛化特性。

### sigmoid

$$
s(x) = \frac{1}{1 + {e}^{-x}}
$$

导数为：
$$
\frac {\partial s(x)}{\partial x} = s(x)(1-s(x))
$$
- sigmoid需要进行指数运算，因此计算速度比较慢
- *函数的输出不是以0为中心，这样会使权重更新效率降低（Why？）*，[参考](https://liam.page/2018/04/17/zero-centered-active-function/)

更新效率即指函数的收敛速度，迭代次数越多，说明模型的收敛速度越慢；反之，迭代次数少，则表明收敛速度快。

深度学习的一般方法是反向传播，即链式法则。
$$
w \leftarrow w - \eta \frac {\partial L}{\partial w}
$$
对某一个神经元，其输入输出关系是
$$
f(\vec x;\vec w, b) = f(\sum w_i x_i + b)
$$
因此，对某一个参数$w_i$来说
$$
\frac {\partial L}{\partial w_i} = \frac {\partial L}{\partial f} \frac {\partial f}{\partial z} \frac {\partial z}{\partial w_i} = x_i \frac {\partial L}{\partial f} \frac {\partial f}{\partial z}
$$
因此，对于某一个参数，其更新过程为
$$
w_i \leftarrow w_i - \eta \frac {\partial L}{\partial w_i} = w_i - \eta x_i \frac {\partial L}{\partial f} \frac {\partial f}{\partial z}
$$

参数$w_i$的实际更新方向由$\frac {\partial L}{\partial f} \frac {\partial f}{\partial z}$决定。另外，从上式也可以看出对于任意一个$w_i$，$\frac {\partial L}{\partial f} \frac {\partial f}{\partial z}$都是常数，所以各个$w_i$更新方向的差异，完全由输入值$x_i$决定。

**以零为中心的影响：**

以二维输入为例
$$
f(\vec x;\vec w, b) = f(w_0 x_0 + w_1 x_1 + b)
$$
现在假设参数$w_0$，$w_1$的最优解是$w_0^*$，$w_1^*$，且满足条件
$$
w_0 \lt w_0^*, w_1 \ge w_1^*
$$
这说明，$w_0$希望增大，而$w_1$需要适当减小。因此要求$x_0$和$x_1$的符号相反，但是在sigmoid函数中，输出值始终未正。这也就是说，如果上一级神经元采用sigmoid函数作为激活函数，那么我们总无法做到$x_0$和$x_1$符号相反。

### tanh

$$
s(x) = \frac {e^z - e^{-z}}{e^z + e^{-z}}
$$

导数为：
$$
\frac {\partial s(x)}{\partial x} = 1 - s(x)^2
$$

当输入远离坐标原点时，同sigmoid函数类似，梯度会变小，使得参数的更新速度下降。

### relu

$$
s(x) = max(0, x)
$$

其导数为
$$
当x > 0时，为1；当x < 0时，为0
$$


### leaky relu

为了解决relu函数在$z \lt 0$时参数不被更新的问题，出现了leaky relu函数
$$
s(x) = x, x \ge 0; s(x) = ax, x \lt 0, a通常是一个很小的值
$$

## 参考文献

- [An overview of gradient descent optimization  algorithm](https://arxiv.org/pdf/1609.04747.pdf)
- [深度学习基础和数学远离](https://github.com/zergtant/pytorch-handbook/blob/master/chapter2/2.2-deep-learning-basic-mathematics.ipynb)
- [激活函数过零点的好处](https://liam.page/2018/04/17/zero-centered-active-function/)